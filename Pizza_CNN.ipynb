{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu7GhXJpA5aI9S1z0Oq3y0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clashgamer123/SOC_Pytorch/blob/main/Pizza_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the google drive on colab to access the data set."
      ],
      "metadata": {
        "id": "UZ5jbt41xwBp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3QJ_MuJm25p",
        "outputId": "7f76267e-86f4-4bd3-da2f-a3fade334e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all the required modules and functions."
      ],
      "metadata": {
        "id": "R5bcnIhgx209"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "from random import shuffle"
      ],
      "metadata": {
        "id": "mA14S4-3nPAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the data using a simple function.\n",
        "Now comes the important part. We need to represent the data as an array or list of tuples with each tuple containg the tensor and the label of the respective image. This is to load the data using DataLoader conveniently."
      ],
      "metadata": {
        "id": "863CxiA9x90Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(path):\n",
        "  data = torch.tensor([])\n",
        "  file_list = os.listdir(path)\n",
        "  for file_name in file_list:\n",
        "    img_path = os.path.join(path, file_name)\n",
        "    image = mpimg.imread(img_path)\n",
        "    image = torch.from_numpy(image)\n",
        "    image = (image - image.min()) / 255\n",
        "    image = image.permute(2, 0, 1)\n",
        "    data = torch.cat((data, image.unsqueeze(0)), 0)\n",
        "  return data\n",
        "\n",
        "pizza_path = '/content/drive/MyDrive/Colab Notebooks/Assignment_1/Data_Set/pizza'\n",
        "not_pizza_path = '/content/drive/MyDrive/Colab Notebooks/Assignment_1/Data_Set/not_pizza'\n",
        "\n",
        "pizza_data = get_data(pizza_path)\n",
        "pizza_size = len(pizza_data)\n",
        "pizza_labels = torch.ones(pizza_size)\n",
        "\n",
        "not_pizza_data = get_data(not_pizza_path)\n",
        "not_pizza_size = len(not_pizza_data)\n",
        "not_pizza_labels = torch.zeros(not_pizza_size)\n",
        "\n",
        "pizza_data_set = []\n",
        "for i in range(pizza_size):\n",
        "  pizza_data_set += [(pizza_data[i], pizza_labels[i])]\n",
        "\n",
        "not_pizza_data_set = []\n",
        "for i in range(not_pizza_size):\n",
        "  not_pizza_data_set += [(not_pizza_data[i], not_pizza_labels[i])]\n",
        "\n"
      ],
      "metadata": {
        "id": "tyz7_J-xoLas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bc38b4-05da-4322-c345-0e7b2b0f04cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-5563041772ab>:7: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  image = torch.from_numpy(image)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our DataSet is quite small,\n",
        "We are going to use DATA AUGMENTATION to artificially increase the size of our data.\n",
        " We mainly use the transforms module in torch.utils.data"
      ],
      "metadata": {
        "id": "Df3soPhVyWbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming you have custom dataset classes for your data\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.data[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "IZv0QzSTyqBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now augment the data, prepare the cumulative data and then load it using DataLoader."
      ],
      "metadata": {
        "id": "42Qi8yg2yvEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Your datasets\n",
        "train_data_set_u = pizza_data_set[0:400] + not_pizza_data_set[0:250]\n",
        "test_data_set_u = pizza_data_set[400:500] + not_pizza_data_set[250:350]\n",
        "\n",
        "# Define transformations\n",
        "transform1 = transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "augment_transform_1 = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "])\n",
        "\n",
        "augment_transform_2 = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.8),\n",
        "    transforms.RandomRotation(degrees=45),\n",
        "])\n",
        "\n",
        "# Create datasets with transformations\n",
        "train_data_set = CustomDataset(train_data_set_u, transform=transform1)\n",
        "test_data_set = CustomDataset(test_data_set_u, transform=transform1)\n",
        "\n",
        "# Augmentations will be applied on-the-fly\n",
        "train_augmented_1 = CustomDataset(train_data_set, transform=augment_transform_1)\n",
        "train_augmented_2 = CustomDataset(train_data_set, transform=augment_transform_2)\n",
        "\n",
        "test_augmented_1 = CustomDataset(test_data_set, transform=augment_transform_1)\n",
        "test_augmented_2 = CustomDataset(test_data_set, transform=augment_transform_2)\n",
        "\n",
        "# Combine original and augmented datasets\n",
        "train_data_set = train_data_set + train_augmented_1 + train_augmented_2\n",
        "test_data_set = test_data_set + test_augmented_1 + test_augmented_2\n",
        "\n",
        "# Create DataLoaders\n",
        "train_data_loader = DataLoader(train_data_set, batch_size=5, shuffle=True)\n",
        "test_data_loader = DataLoader(test_data_set, batch_size=20, shuffle=True)\n"
      ],
      "metadata": {
        "id": "SWIYFRB9uc9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define our model using the nn.Sequential().\n",
        "We are using 3 convolutional layers.\n",
        "BatchNorm2d normalizes the input to each layer by subtracting the mean and dividing by the standard deviation of the activations in a mini-batch. This ensures that the inputs to each layer have a mean of zero and a standard deviation of one, which helps mitigate issues like vanishing and exploding gradients."
      ],
      "metadata": {
        "id": "FGw1yUaay8ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 32, kernel_size=3, stride = 1, padding = 1),\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "    nn.Conv2d(32, 64, kernel_size=3, stride = 1, padding = 1),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "    nn.Conv2d(64, 128, kernel_size=3, stride = 1, padding = 1),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(16*16*128, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "Jdmitk5XuoQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "vZ5gKZCYxl7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "cyve3KxLzj8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "  cum_loss = 0\n",
        "  for data in train_data_loader:\n",
        "    inputs, labels = data\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_criterion(outputs, labels.unsqueeze(1).float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    cum_loss += loss*len(inputs)\n",
        "  cum_loss = cum_loss / 1950\n",
        "  print(f'Loss at epoch: {epoch+1} :  {cum_loss.item()}')"
      ],
      "metadata": {
        "id": "1FY16JpDxsXZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "9dd5b7a8-259a-48f5-fad8-534d78d0d185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f4d8560c27b2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mcum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcum_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1950\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                             )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 state_steps)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    319\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_matches = 0\n",
        "for data, labels in test_data_loader:\n",
        "  outputs = model(data)\n",
        "  outputs = outputs.squeeze()\n",
        "  outputs = (outputs>=0.5).int()\n",
        "  correct_matches += (outputs == labels).sum()\n",
        "\n",
        "print(f'Accuracy of the model = {(correct_matches/600)*100:0.4f}')\n",
        "\n",
        "# Accuracy upto 90 percent can be achieved."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RjkxMj12Rmi",
        "outputId": "82730ede-b96f-4307-d58f-a71def64c2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model = 85.6667\n"
          ]
        }
      ]
    }
  ]
}